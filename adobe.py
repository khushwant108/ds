# -*- coding: utf-8 -*-
"""adobe.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b6H1BhEYpI51zvKGR-AJT9GvkSt2FXQe
"""

import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration
from sqlalchemy import create_engine

# Load the NL to SQL and Query Correction models
nl_to_sql_model = T5ForConditionalGeneration.from_pretrained("t5-small")
query_correction_model = T5ForConditionalGeneration.from_pretrained("t5-small")
tokenizer = T5Tokenizer.from_pretrained("t5-small")

# Connect to PostgreSQL
engine = create_engine('postgresql://username:password@localhost:5432/database_name')

def generate_sql(natural_language_query):
    """
    Generate SQL query from natural language input.
    """
    inputs = tokenizer(natural_language_query, return_tensors="pt", padding=True, truncation=True)
    outputs = nl_to_sql_model.generate(**inputs)
    sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return sql_query

def correct_sql(incorrect_sql_query):
    """
    Correct an SQL query.
    """
    inputs = tokenizer(incorrect_sql_query, return_tensors="pt", padding=True, truncation=True)
    outputs = query_correction_model.generate(**inputs)
    corrected_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return corrected_sql

def execute_query(sql_query):
    """
    Execute SQL query on PostgreSQL.
    """
    try:
        result = engine.execute(sql_query)
        return result.fetchall()
    except Exception as e:
        return f"Error executing query: {e}"

# Example usage
if __name__ == "__main__":
    # Step 1: Generate SQL from natural language
    natural_language_query = "Show me all users who joined in 2023"
    sql_query = generate_sql(natural_language_query)
    print(f"Generated SQL: {sql_query}")

    # Step 2: Correct SQL (if needed)
    corrected_sql = correct_sql(sql_query)
    print(f"Corrected SQL: {corrected_sql}")

    # Step 3: Execute the query
    result = execute_query(corrected_sql)
    print(f"Query Result: {result}")

import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration

def train_nl_to_sql_model():
    """
    Train the NL to SQL model using the provided dataset.
    """
    # Load the dataset
    df = pd.read_csv('train_generate_task.csv')

    # Initialize the tokenizer and model
    tokenizer = T5Tokenizer.from_pretrained("t5-small")
    model = T5ForConditionalGeneration.from_pretrained("t5-small")

    # Preprocess the data
    inputs = tokenizer(df['NL'].tolist(), return_tensors="pt", padding=True, truncation=True)
    labels = tokenizer(df['Query'].tolist(), return_tensors="pt", padding=True, truncation=True)

    # Fine-tune the model (simplified training loop)
    # Note: In practice, you would use a proper training loop with epochs, loss calculation, etc.
    model.train()
    outputs = model(input_ids=inputs['input_ids'], labels=labels['input_ids'])
    loss = outputs.loss
    print(f"Training Loss: {loss}")

    # Save the model
    model.save_pretrained("train_generate_task.json")
    tokenizer.save_pretrained("train_generate_task.json")

if __name__ == "__main__":
    train_nl_to_sql_model()

import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration

def train_query_correction_model():
    """
    Train the Query Correction model using the provided dataset.
    """
    # Load the dataset
    df = pd.read_csv('train_query_correction_task.csv')

    # Initialize the tokenizer and model
    tokenizer = T5Tokenizer.from_pretrained("t5-small")
    model = T5ForConditionalGeneration.from_pretrained("t5-small")

    # Preprocess the data
    inputs = tokenizer(df['IncorrectQuery'].tolist(), return_tensors="pt", padding=True, truncation=True)
    labels = tokenizer(df['CorrectQuery'].tolist(), return_tensors="pt", padding=True, truncation=True)

    # Fine-tune the model (simplified training loop)
    model.train()
    outputs = model(input_ids=inputs['input_ids'], labels=labels['input_ids'])
    loss = outputs.loss
    print(f"Training Loss: {loss}")

    # Save the model
    model.save_pretrained("train_query_correction_task")
    tokenizer.save_pretrained("train_query_correction_task")

if __name__ == "__main__":
    train_query_correction_model()

from sqlalchemy import create_engine

def connect_to_postgres():
    """
    Connect to PostgreSQL database.
    """
    engine = create_engine('postgresql://postgress:sql@khush@localhost:5432/Devstrom')
    return engine

!pip install groq

import groq

def initialize_groq_client(api_key):
    """
    Initialize GROQ client with API key.
    """
    client = groq.Client(api_key=api_key)
    return client